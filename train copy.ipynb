{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#The following instructions can be used to train a Transformer model on the [IWSLT'14 German to English dataset](http://workshop2014.iwslt.org/downloads/proceeding.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1711129868677 failed, reason: connect ECONNREFUSED 127.0.0.1:8888).)."
     ]
    }
   ],
   "source": [
    "\n",
    "%%bash \n",
    "\n",
    "cd /mnt/c/translator\n",
    "cd examples/translation/\n",
    "bash prepare-wmt14en2de.sh\n",
    "cd ../..\n",
    "\n",
    "TEXT=examples/translation/wmt17_en_de\n",
    "fairseq-preprocess ^\n",
    "    --source-lang en --target-lang de \\\n",
    "    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n",
    "    --destdir data-bin/wmt17_en_de --thresholdtgt 0 --thresholdsrc 0 \\\n",
    "    --workers 20\n",
    "    \n",
    "mkdir -p checkpoints/fconv_wmt_en_de\n",
    "# fairseq-train \\\n",
    "#     data-bin/wmt17_en_de \\\n",
    "#     --arch fconv_wmt_en_de \\\n",
    "#     --dropout 0.2 \\\n",
    "#     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "#     --optimizer nag --clip-norm 0.1 \\\n",
    "#     --lr 0.5 --lr-scheduler fixed --force-anneal 50 \\\n",
    "#     --max-tokens 4000 \\\n",
    "#     --save-dir checkpoints/fconv_wmt_en_de\n",
    "# alias python=python3\n",
    "\n",
    "\n",
    "    \n",
    "# cd examples/translation/\n",
    "# bash wmt14en-de.sh\n",
    "# cd ../..\n",
    "\n",
    "# # Preprocess/binarize the data\n",
    "# TEXT=examples/translation/wmt14en2de.tokenized.de-en\n",
    "# fairseq-preprocess --source-lang de --target-lang en \\\n",
    "#     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n",
    "#     --destdir data-bin/wmt14en2de.tokenized.de-en \\\n",
    "#     --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 541, in cli_main\n",
      "    parser = options.get_training_parser()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/options.py\", line 38, in get_training_parser\n",
      "    parser = get_parser(\"Trainer\", default_task)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/options.py\", line 234, in get_parser\n",
      "    utils.import_user_module(usr_args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/utils.py\", line 491, in import_user_module\n",
      "    importlib.import_module(module_name)\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/mnt/c/translator/models/transformer_legacy.py\", line 11, in <module>\n",
      "    from models.transformer_config import (\n",
      "ModuleNotFoundError: No module named 'models'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'CUDA_VISIBLE_DEVICES=0 fairseq-train \\\\\\n    data-bin/wmt17_en_de \\\\\\n    --arch meta-transformer_wmt_de_en --share-decoder-input-output-embed \\\\\\n    --user-dir models/transformer_legacy.py\\\\\\n    --optimizer adam --adam-betas \\'(0.9, 0.98)\\' --clip-norm 0.0 \\\\\\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\\\\n    --dropout 0.3 --weight-decay 0.0001 \\\\\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\\\\n    --max-tokens 500 \\\\\\n    --eval-bleu \\\\\\n    --eval-bleu-args \\'{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}\\' \\\\\\n    --eval-bleu-detok moses \\\\\\n    --eval-bleu-remove-bpe \\\\\\n    --eval-bleu-print-samples \\\\\\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCUDA_VISIBLE_DEVICES=0 fairseq-train \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data-bin/wmt17_en_de \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --arch meta-transformer_wmt_de_en --share-decoder-input-output-embed \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --user-dir models/transformer_legacy.py\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --optimizer adam --adam-betas \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m(0.9, 0.98)\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m --clip-norm 0.0 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --dropout 0.3 --weight-decay 0.0001 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --max-tokens 500 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --eval-bleu \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --eval-bleu-args \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbeam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: 5, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_len_a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: 1.2, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_len_b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: 10}\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --eval-bleu-detok moses \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --eval-bleu-remove-bpe \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --eval-bleu-print-samples \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:155\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:315\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'CUDA_VISIBLE_DEVICES=0 fairseq-train \\\\\\n    data-bin/wmt17_en_de \\\\\\n    --arch meta-transformer_wmt_de_en --share-decoder-input-output-embed \\\\\\n    --user-dir models/transformer_legacy.py\\\\\\n    --optimizer adam --adam-betas \\'(0.9, 0.98)\\' --clip-norm 0.0 \\\\\\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\\\\n    --dropout 0.3 --weight-decay 0.0001 \\\\\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\\\\n    --max-tokens 500 \\\\\\n    --eval-bleu \\\\\\n    --eval-bleu-args \\'{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}\\' \\\\\\n    --eval-bleu-detok moses \\\\\\n    --eval-bleu-remove-bpe \\\\\\n    --eval-bleu-print-samples \\\\\\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# %%bash \n",
    "# CUDA_VISIBLE_DEVICES=0 fairseq-train \\\n",
    "#     data-bin/wmt17_en_de \\\n",
    "#     --arch meta_transformer_wmt_de_en --share-decoder-input-output-embed \\\n",
    "#     --user-dir transformer\\\n",
    "#     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "#     --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "#     --dropout 0.3 --weight-decay 0.0001 \\\n",
    "#     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "#     --max-tokens 500 \\\n",
    "#     --eval-bleu \\\n",
    "#     --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "#     --eval-bleu-detok moses \\\n",
    "#     --eval-bleu-remove-bpe \\\n",
    "#     --eval-bleu-print-samples \\\n",
    "#     --best-checkpoint-metric bleu --maximize-best-checkpoint-metric\n",
    "    fairseq-train ^\n",
    "    data-bin/wmt17_en_de ^\n",
    "    --arch meta_transformer_wmt_de_en --share-decoder-input-output-embed ^\n",
    "    --user-dir transformer^\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 ^\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 ^\n",
    "    --dropout 0.3 --weight-decay 0.0001 ^\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 ^\n",
    "    --max-tokens 500 ^\n",
    "    --eval-bleu ^\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' ^\n",
    "    --eval-bleu-detok moses ^\n",
    "    --eval-bleu-remove-bpe ^\n",
    "    --eval-bleu-print-samples ^\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "fairseq-generate data-bin/iwslt14.tokenized.de-en \\\n",
    "    --path checkpoints/checkpoint_best.pt \\\n",
    "    --batch-size 128 --beam 5 --remove-bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\18816/.cache\\torch\\hub\\pytorch_fairseq_main\n",
      " 43%|████▎     | 5165261824/11946275315 [05:06<06:42, 16847320.76B/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load an En-Fr Transformer model trained on WMT'14 data :\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m en2de \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpytorch/fairseq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransformer.wmt19.en-de\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmoses\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbpe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfastbpe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m de2en \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch/fairseq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.wmt19.de-en\u001b[39m\u001b[38;5;124m'\u001b[39m, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoses\u001b[39m\u001b[38;5;124m'\u001b[39m, bpe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastbpe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m paraphrase \u001b[38;5;241m=\u001b[39m de2en\u001b[38;5;241m.\u001b[39mtranslate(en2de\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPyTorch Hub is an awesome interface!\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\18816\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\hub.py:558\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    555\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    556\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[1;32m--> 558\u001b[0m model \u001b[38;5;241m=\u001b[39m _load_local(repo_or_dir, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\18816\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\hub.py:587\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    584\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m    586\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[1;32m--> 587\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\pytorch_fairseq_main\\fairseq\\models\\fairseq_model.py:272\u001b[0m, in \u001b[0;36mBaseFairseqModel.from_pretrained\u001b[1;34m(cls, model_name_or_path, checkpoint_file, data_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03mLoad a :class:`~fairseq.models.FairseqModel` from a pre-trained model\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03mfile. Downloads and caches the pre-trained model file if needed.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03m        model archive path.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hub_utils\n\u001b[1;32m--> 272\u001b[0m x \u001b[38;5;241m=\u001b[39m hub_utils\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    273\u001b[0m     model_name_or_path,\n\u001b[0;32m    274\u001b[0m     checkpoint_file,\n\u001b[0;32m    275\u001b[0m     data_name_or_path,\n\u001b[0;32m    276\u001b[0m     archive_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mhub_models(),\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    278\u001b[0m )\n\u001b[0;32m    279\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hub_utils\u001b[38;5;241m.\u001b[39mGeneratorHubInterface(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m], x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m], x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\pytorch_fairseq_main\\fairseq\\hub_utils.py:56\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(model_name_or_path, checkpoint_file, data_name_or_path, archive_map, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# convenience hack for loading data and BPE codes from model archive\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_name_or_path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 56\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_name_or_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m file_utils\u001b[38;5;241m.\u001b[39mload_archive_file(data_name_or_path)\n",
      "File \u001b[1;32mc:\\Users\\18816\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ntpath.py:104\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(path, \u001b[38;5;241m*\u001b[39mpaths):\n\u001b[1;32m--> 104\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m    106\u001b[0m         sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load an En-Fr Transformer model trained on WMT'14 data :\n",
    "en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de', tokenizer='moses', bpe='fastbpe')\n",
    "de2en = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.de-en', tokenizer='moses', bpe='fastbpe')\n",
    "\n",
    "paraphrase = de2en.translate(en2de.translate('PyTorch Hub is an awesome interface!'))\n",
    "assert paraphrase == 'PyTorch Hub is a fantastic interface!'\n",
    "\n",
    "# Compare the results with English-Russian round-trip translation:\n",
    "en2ru = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-ru.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "ru2en = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.ru-en.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "\n",
    "paraphrase = ru2en.translate(en2ru.translate('PyTorch Hub is an awesome interface!'))\n",
    "assert paraphrase == 'PyTorch is a great interface!'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
