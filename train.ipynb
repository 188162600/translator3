{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#The following instructions can be used to train a Transformer model on the [IWSLT'14 German to English dataset](http://workshop2014.iwslt.org/downloads/proceeding.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1711129868677 failed, reason: connect ECONNREFUSED 127.0.0.1:8888).)."
     ]
    }
   ],
   "source": [
    "\n",
    "%%bash \n",
    "\n",
    "cd /mnt/c/translator\n",
    "cd examples/translation/\n",
    "bash prepare-wmt14en2de.sh\n",
    "cd ../..\n",
    "\n",
    "TEXT=examples/translation/wmt17_en_de\n",
    "fairseq-preprocess \\\n",
    "    --source-lang en --target-lang de \\\n",
    "    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n",
    "    --destdir data-bin/wmt17_en_de --thresholdtgt 0 --thresholdsrc 0 \\\n",
    "    --workers 20\n",
    "    \n",
    "mkdir -p checkpoints/fconv_wmt_en_de\n",
    "# fairseq-train \\\n",
    "#     data-bin/wmt17_en_de \\\n",
    "#     --arch fconv_wmt_en_de \\\n",
    "#     --dropout 0.2 \\\n",
    "#     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "#     --optimizer nag --clip-norm 0.1 \\\n",
    "#     --lr 0.5 --lr-scheduler fixed --force-anneal 50 \\\n",
    "#     --max-tokens 4000 \\\n",
    "#     --save-dir checkpoints/fconv_wmt_en_de\n",
    "# alias python=python3\n",
    "\n",
    "\n",
    "    \n",
    "# cd examples/translation/\n",
    "# bash wmt14en-de.sh\n",
    "# cd ../..\n",
    "\n",
    "# # Preprocess/binarize the data\n",
    "# TEXT=examples/translation/wmt14en2de.tokenized.de-en\n",
    "# fairseq-preprocess --source-lang de --target-lang en \\\n",
    "#     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n",
    "#     --destdir data-bin/wmt14en2de.tokenized.de-en \\\n",
    "#     --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "CUDA_VISIBLE_DEVICES=0 fairseq-train \\\n",
    "    data-bin/wmt17_en_de \\\n",
    "    --arch meta-transformer_wmt_de_en --share-decoder-input-output-embed \\\n",
    "    --user-dir models/transformer_legacy.py\\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --eval-bleu \\\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "    --eval-bleu-detok moses \\\n",
    "    --eval-bleu-remove-bpe \\\n",
    "    --eval-bleu-print-samples \\\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "fairseq-generate data-bin/iwslt14.tokenized.de-en \\\n",
    "    --path checkpoints/checkpoint_best.pt \\\n",
    "    --batch-size 128 --beam 5 --remove-bpe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
